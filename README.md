# ğŸ“Œ Project Overview
This project is an automated web scraper that extracts the latest articles from Hacker News using Playwright.</br>
It checks whether the articles are sorted from newest to oldest, saves them in structured files, and provides multiple execution options.</br>

The script:</br>
âœ… Extracts article titles, submission IDs, and timestamps.</br>
âœ… Validates sorting order using submission IDs.</br>
âœ… Handles pagination dynamically.</br>
âœ… Exports data to CSV & JSON for easy analysis.</br>
âœ… Supports flexible execution options (--headless, --limit).</br>

# ğŸ“Œ Project Structure
QA_WOLF_TAKE_HOME
â”‚â”€â”€ node_modules/         # Dependencies (not included in GitHub)</br>
â”‚â”€â”€ .gitignore            # Ignore unnecessary files</br>
â”‚â”€â”€ hacker_news_articles_<timestamp>.csv  # Extracted articles (CSV format)</br>
â”‚â”€â”€ hacker_news_articles_<timestamp>.json # Extracted articles (JSON format)</br>
â”‚â”€â”€ index.js              # Main script</br>
â”‚â”€â”€ package.json          # Node.js package file</br>
â”‚â”€â”€ package-lock.json     # Package dependencies</br>
â”‚â”€â”€ playwright.config.js  # Playwright configuration</br>
â”‚â”€â”€ README.md             # Project documentation (this file)</br>

# ğŸ“Œ Installation Instructions
1ï¸âƒ£ Prerequisites</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Install Node.js (v14+ recommended)</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Install Playwright (if not installed)</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;npm install playwright</br>

2ï¸âƒ£ Clone the Repository</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;git clone https://github.com/YOUR_GITHUB_USERNAME/qa-wolf-take-home.git</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cd qa-wolf-take-home</br>

3ï¸âƒ£ Install Dependencies</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;npm install</br>

# ğŸ“Œ How to Run the Script
The script provides multiple execution options for flexibility:</br>

1ï¸âƒ£ Default Execution (Fetches 100 Articles in Visible Mode)</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node index.js</br>
âœ” Runs Playwright with a visible browser.</br>
âœ” Fetches 100 articles from Hacker News.</br>
âœ” Saves data to CSV & JSON files.</br>

2ï¸âƒ£ Run in Headless Mode (Faster Execution)</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node index.js --headless</br>
âœ” Runs without opening a browser window.</br>
âœ” Useful for CI/CD pipelines & faster execution.</br>

3ï¸âƒ£ Fetch a Custom Number of Articles (e.g., 50)</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node index.js --limit 50</br>
âœ” Retrieves 50 articles instead of 100.</br>
âœ” Saves output in structured files.</br>

4ï¸âƒ£ Fetch More Articles in Headless Mode
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node index.js --limit 200 --headless</br>
âœ” Fetches 200 articles.</br>
âœ” Runs without opening a browser for maximum speed.</br>

# ğŸ“Œ Expected Output
The extracted articles are saved in CSV & JSON files, with a timestamp to prevent overwriting.</br>

1ï¸âƒ£ CSV Output (hacker_news_articles_<timestamp>.csv)</br>
S.No,Title,Submission ID,Time</br>
1,"AI Breakthrough in Healthcare",39543218,"2 minutes ago"</br>
2,"New JavaScript Framework Released",39543216,"5 minutes ago"</br>
3,"NASAâ€™s Latest Discovery",39543214,"8 minutes ago"</br>
...</br>
ğŸ“‚ Location: Same directory as the script.</br>

2ï¸âƒ£ JSON Output (hacker_news_articles_<timestamp>.json)</br>

[</br>
  {</br>
    "S.No": 1,</br>
    "title": "AI Breakthrough in Healthcare",</br>
    "submissionId": 39543218,</br>
    "time": "2 minutes ago"</br>
  },</br>
  {</br>
    "S.No": 2,</br>
    "title": "New JavaScript Framework Released",</br>
    "submissionId": 39543216,</br>
    "time": "5 minutes ago"</br>
  }</br>
]</br>
ğŸ“‚ Location: Same directory as the script.</br>

# ğŸ“Œ How the Script Works</br>
| **Step** | **Description** |
|----------|---------------|
| **1** | Launches a Chromium browser and navigates to Hacker News ["Newest"](https://news.ycombinator.com/newest) page. |
| **2** | Extracts up to **N** articles (**default: 100**). |
| **3** | Handles **pagination dynamically** if more articles are needed. |
| **4** | Validates sorting using **submission IDs** to ensure articles are arranged from **newest to oldest**. |
| **5** | Saves extracted data to **CSV & JSON** files with dynamically generated filenames. |
| **6** | Displays the first **10 extracted articles** in the console for verification. |
| **7** | Closes the **browser session** after successful execution. |

# ğŸ“Œ Key Features & Optimizations</br>
âœ” Supports Headless & Visible Mode â†’ Flexible execution options.</br>
âœ” Handles Pagination Automatically â†’ Fetches multiple pages if needed.</br>
âœ” Parallelized Data Extraction â†’ Uses Playwrightâ€™s fast element selection.</br>
âœ” Error Handling & Robust Execution â†’ Skips missing data, prevents crashes.</br>
âœ” Dynamically Named Output Files â†’ Prevents overwriting, keeps data organized.</br>
âœ” Command-Line Customization â†’ Fetch any number of articles with --limit.</br>

